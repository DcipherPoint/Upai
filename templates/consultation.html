<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Consultation - {{ patient.name }}</title>
    <style>
        body { font-family: sans-serif; line-height: 1.6; padding: 20px; max-width: 800px; margin: auto; background-color: #f4f7f6; }
        .container { background-color: #fff; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        h1, h2 { color: #0056b3; text-align: center; margin-bottom: 20px; }
        h3 { color: #333; margin-top: 25px; border-bottom: 1px solid #ccc; padding-bottom: 5px; }
        h4 { margin-bottom: 5px; margin-top: 15px; color: #444; }
        .patient-info { text-align: center; font-size: 1.2em; margin-bottom: 15px; color: #555; }
        .controls button, .actions button, .live-controls button { background-color: #007bff; color: white; border: none; padding: 10px 15px; border-radius: 5px; cursor: pointer; font-size: 1em; margin: 5px; transition: background-color 0.3s; }
        .controls button:disabled, .actions button:disabled, .live-controls button:disabled { background-color: #cccccc; cursor: not-allowed; }
        .controls button:hover:not(:disabled) { background-color: #0056b3; }
        .actions button.confirm { background-color: #28a745; }
        .actions button.confirm:hover:not(:disabled) { background-color: #218838; }
        .actions button.download { background-color: #17a2b8; display: none; /* Hidden initially */ }
        .actions button.download:hover:not(:disabled) { background-color: #138496; }
        .live-controls button.start-live { background-color: #ffc107; color: #333; }
        .live-controls button.start-live:hover:not(:disabled) { background-color: #e0a800; }
        .live-controls button.stop-live { background-color: #dc3545; }
        .live-controls button.stop-live:hover:not(:disabled) { background-color: #c82333; }
        .display-area { margin-top: 10px; background-color: #e9ecef; padding: 15px; border-radius: 5px; min-height: 100px; /* Increased height */ white-space: pre-wrap; font-family: monospace; overflow-y: auto; /* Added scroll */ }
        textarea { width: 95%; min-height: 150px; margin-top: 10px; padding: 10px; border: 1px solid #ccc; border-radius: 4px; font-size: 1em; }
        #loadingIndicator, #liveStatus { display: none; text-align: center; margin-top: 15px; font-weight: bold; color: #dc3545; }
        #statusMessage { text-align: center; margin-top: 10px; color: #6c757d; }
        #errorDisplay, #liveErrorDisplay { color: #dc3545; font-weight: bold; margin-top: 15px; text-align: center; min-height: 1.2em; }
        .button-group { text-align: center; margin-bottom: 20px; }
        label { font-weight: bold; margin-right: 5px; }
        select { padding: 5px; border-radius: 4px; }
        .section { margin-bottom: 30px; padding-bottom: 20px; border-bottom: 1px dashed #ddd; }
        .live-transcript-section { background-color: #fff8e1; padding: 15px; border-radius: 5px; border: 1px solid #ffeeba; }
        #audioVisualizer { width: 100%; height: 60px; background-color: #333; display: block; margin-top: 10px; border-radius: 4px; }
        #activeMicDisplay { font-style: italic; color: #555; text-align: center; margin-top: 5px; font-size: 0.9em; min-height: 1em; }
        #processingIndicator { display: none; text-align: center; margin-top: 15px; font-weight: bold; color: #ffc107; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Smart Care Assistant - Upai</h1>
        <h2>Consultation Record</h2>
        <div class="patient-info">Patient: <strong>{{ patient.name }}</strong> (ID: {{ patient_id }})</div>

        <!-- Audio Input Selection -->
        <div class="button-group audio-selection">
            <label for="audioSource">Select Microphone:</label>
            <select id="audioSource">
                <option value="default">Default Microphone</option>
            </select>
            <div id="activeMicDisplay"></div> <!-- Display for active mic -->
        </div>
        <hr>

        <!-- Combined Live Recording & Processing Section -->
        <div>
            <h3>Record & Process Consultation</h3>
             <p style="font-size: 0.9em; color: #6c757d; text-align: center; margin-bottom: 10px;">
                <i>Click "Start Recording", dictate, then click "Stop & Process". The transcript will appear below, followed by the AI draft.</i>
            </p>
            <div id="liveStatus">Ready to record.</div>
            <div id="liveErrorDisplay"></div>
            <div class="button-group live-controls">
                <!-- Renamed buttons -->
                <button id="startRecordingButton" class="start-live">Start Recording</button>
                <button id="stopAndProcessButton" class="stop-live" disabled>Stop & Process</button>
            </div>
            <h4>Live Audio Input Visualizer</h4>
            <canvas id="audioVisualizer"></canvas>
            <h4>Live Transcript (Finalized)</h4>
            <div id="liveTranscriptArea" class="display-area">(Transcript will appear here as you speak)</div>

            <div id="processingIndicator">Processing transcript with AI...</div>

            <h4>AI Generated Draft (Editable)</h4>
            <textarea id="aiDraftArea" placeholder="AI summary and prescription draft will appear here after processing."></textarea>
            <!-- <input type="hidden" id="initialAiSummary"> Removed, no longer needed -->

            <div class="button-group actions">
                <button id="confirmButton" class="confirm" disabled>Confirm & Save Consultation</button>
                <button id="downloadButton" class="download">Download PDF</button>
            </div>
        </div>

    </div>

    <script>
        // --- Element References (Adjusted) ---
        // REMOVED: startButton, stopButton, transcriptArea, loadingIndicator, statusMessage, errorDisplay
        const confirmButton = document.getElementById('confirmButton');
        const downloadButton = document.getElementById('downloadButton');
        const aiDraftArea = document.getElementById('aiDraftArea');
        const audioSourceSelect = document.getElementById('audioSource');
        const activeMicDisplay = document.getElementById('activeMicDisplay');

        const startRecordingButton = document.getElementById('startRecordingButton'); // Renamed
        const stopAndProcessButton = document.getElementById('stopAndProcessButton'); // Renamed
        const liveTranscriptArea = document.getElementById('liveTranscriptArea');
        const liveStatus = document.getElementById('liveStatus');
        const liveErrorDisplay = document.getElementById('liveErrorDisplay');
        const canvas = document.getElementById('audioVisualizer');
        const canvasCtx = canvas.getContext('2d');
        const processingIndicator = document.getElementById('processingIndicator'); // New indicator

        // --- Variables (Adjusted) ---
        const patientId = "{{ patient_id }}";
        const doctorId = "{{ doctor_id }}";
        // REMOVED: mediaRecorderMain, audioChunksMain
        let currentConsultationId = null;
        let liveWebSocket = null;
        let liveStream = null;
        let accumulatedFinalTranscript = ""; // Variable to store final transcript parts

        // Web Audio API variables
        let audioCtx = null;
        let analyser = null;
        let source = null;
        let scriptNode = null;
        let dataArray = null;
        let bufferLength = 0;
        let drawVisual = null;
        const SCRIPT_PROCESSOR_BUFFER_SIZE = 4096;

        // --- Initialization ---
        document.addEventListener('DOMContentLoaded', populateAudioDevices);

        // --- Audio Device Selection (Unchanged) ---
        async function populateAudioDevices() {
            activeMicDisplay.textContent = 'Checking permissions...';
            try {
                // Need to get permission first to enumerate devices with labels
                // Use a temporary stream
                const tempStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                // Stop the tracks immediately after permission is granted
                tempStream.getTracks().forEach(track => track.stop());

                const devices = await navigator.mediaDevices.enumerateDevices();
                const audioInputDevices = devices.filter(device => device.kind === 'audioinput');

                audioSourceSelect.innerHTML = ''; // Clear existing options
                if (audioInputDevices.length === 0) {
                     audioSourceSelect.innerHTML = '<option value="default">No microphones found</option>';
                     activeMicDisplay.textContent = 'No microphones detected.';
                     return;
                }

                let defaultSelected = false;
                audioInputDevices.forEach((device, index) => {
                    const option = document.createElement('option');
                    option.value = device.deviceId;
                    option.text = device.label || `Microphone ${index + 1}`;
                    // Select the first non-default device if available
                    if (device.deviceId !== 'default' && !defaultSelected) {
                        // option.selected = true; // Optionally pre-select first real mic
                        defaultSelected = true;
                    }
                    audioSourceSelect.appendChild(option);
                });
                updateActiveMicDisplay(); // Update display initially
                audioSourceSelect.addEventListener('change', updateActiveMicDisplay);

            } catch (err) {
                console.error('Error enumerating audio devices:', err);
                audioSourceSelect.innerHTML = '<option value="default">Error finding devices</option>';
                activeMicDisplay.textContent = 'Error: Check mic permissions.';
                errorDisplay.textContent = 'Could not list microphones. Grant permission and refresh.';
                liveErrorDisplay.textContent = 'Could not list microphones.';
            }
        }

        function getSelectedAudioConstraints() {
            const deviceId = audioSourceSelect.value;
            if (deviceId && deviceId !== 'default') {
                return { audio: { deviceId: { exact: deviceId } } };
            }
            return { audio: true }; // Default device
        }

        function updateActiveMicDisplay() {
            const selectedOption = audioSourceSelect.options[audioSourceSelect.selectedIndex];
            if (selectedOption) {
                activeMicDisplay.textContent = `Using: ${selectedOption.text}`;
            } else {
                activeMicDisplay.textContent = 'Select a microphone.';
            }
        }

        // --- Main Recording & Processing Logic (Replaces Live Demo Logic) ---
        startRecordingButton.addEventListener('click', async () => {
            // REMOVED: Check for mediaRecorderMain state
            if (scriptNode) { // Check if already recording via scriptNode
                 liveErrorDisplay.textContent = 'Already recording.';
                 return;
            }
            updateActiveMicDisplay();
            const constraints = {
                audio: {
                    ...(getSelectedAudioConstraints().audio),
                    sampleRate: 16000,
                    channelCount: 1
                }
            };

            try {
                liveStream = await navigator.mediaDevices.getUserMedia(constraints);
                console.log("Live Mic Stream obtained.");

                // Reset transcript areas
                liveTranscriptArea.textContent = '(Listening...)';
                aiDraftArea.value = '';
                accumulatedFinalTranscript = ""; // Reset accumulator
                liveErrorDisplay.textContent = '';
                confirmButton.disabled = true;
                downloadButton.style.display = 'none';

                // --- Setup Web Audio API ---
                if (!audioCtx) {
                    audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                } else if (audioCtx.state === 'suspended') {
                    await audioCtx.resume();
                }
                console.log(`AudioContext sample rate: ${audioCtx.sampleRate}`);
                if (audioCtx.sampleRate !== 16000 && audioCtx.sampleRate !== 48000) {
                    // Adjusted warning for common rates
                    console.warn(`Warning: AudioContext using ${audioCtx.sampleRate} Hz. Backend expects ${STREAMING_RATE} Hz. STT might fail.`);
                }

                analyser = audioCtx.createAnalyser();
                analyser.fftSize = 256;
                bufferLength = analyser.frequencyBinCount;
                dataArray = new Uint8Array(bufferLength);
                source = audioCtx.createMediaStreamSource(liveStream);
                scriptNode = audioCtx.createScriptProcessor(SCRIPT_PROCESSOR_BUFFER_SIZE, 1, 1);

                scriptNode.onaudioprocess = (audioProcessingEvent) => {
                    const inputBuffer = audioProcessingEvent.inputBuffer;
                    const channelData = inputBuffer.getChannelData(0);
                    const pcm16Buffer = convertFloat32ToInt16(channelData);
                    if (liveWebSocket && liveWebSocket.readyState === WebSocket.OPEN) {
                        liveWebSocket.send(pcm16Buffer);
                    }
                };
                source.connect(analyser);
                source.connect(scriptNode);
                // --- End Web Audio Setup ---

                // Setup WebSocket connection
                const wsProtocol = window.location.protocol === 'https:' ? 'wss://' : 'ws://';
                const wsUrl = `${wsProtocol}${window.location.host}/live_transcript`;
                liveWebSocket = new WebSocket(wsUrl);
                liveWebSocket.binaryType = 'arraybuffer';

                liveWebSocket.onopen = () => {
                    console.log('WebSocket connected.');
                    liveStatus.textContent = `Recording... (Mic: ${audioSourceSelect.options[audioSourceSelect.selectedIndex]?.text || 'Default'})`;
                    liveStatus.style.display = 'block';
                    startRecordingButton.disabled = true;
                    stopAndProcessButton.disabled = false;
                    visualize(); // Start visualization
                };

                liveWebSocket.onmessage = (event) => {
                    console.log("Raw WS Message Received:", event.data);
                    const message = event.data;
                    const liveArea = document.getElementById('liveTranscriptArea');
                    const liveErrDisp = document.getElementById('liveErrorDisplay');
                    if (!liveArea || !liveErrDisp) return;

                    // Display interim results slightly differently if desired
                    if (message.startsWith('INTERIM:')) {
                        const interimTranscript = message.substring(8);
                        // Simple display: overwrite content with latest interim
                        liveArea.textContent = interimTranscript + "...";
                        liveErrDisp.textContent = '';
                    } else if (message.startsWith('FINAL:')) {
                        const finalTranscript = message.substring(6).trim();
                        if (finalTranscript) { // Only add non-empty final transcripts
                            accumulatedFinalTranscript += finalTranscript + "\n"; // Append with newline
                        }
                        liveArea.textContent = accumulatedFinalTranscript; // Show accumulated text
                        liveArea.scrollTop = liveArea.scrollHeight; // Scroll to bottom
                        liveErrDisp.textContent = '';
                    } else if (message.startsWith('ERROR:')) {
                        liveErrDisp.textContent = `Live Error: ${message.substring(6)}`;
                        // Consider stopping on error? stopRecordingAndProcess();
                    } else if (message.startsWith('STATUS:')) {
                        liveErrDisp.textContent = `Status: ${message.substring(7)}`;
                    }
                };

                liveWebSocket.onerror = (error) => {
                    console.error('WebSocket Error:', error);
                    liveErrorDisplay.textContent = 'WebSocket connection error.';
                    stopRecordingAndProcess(true); // Pass error flag
                };

                liveWebSocket.onclose = (event) => {
                    console.log('WebSocket closed:', event.code, event.reason);
                    // Don't reset status here if processing is intended
                    if (startRecordingButton.disabled) { // Only update status if we were recording
                       // liveStatus.textContent = `Connection closed (${event.code}). Processing...`;
                    } else {
                         liveStatus.textContent = 'Recording stopped.';
                    }
                    // Ensure cleanup happens if closed unexpectedly
                    if (scriptNode) { // Check if scriptNode still exists (means stop wasn't called cleanly)
                        stopRecordingAndProcess(true); // Trigger cleanup
                    }
                };

            } catch (err) {
                console.error('Error starting recording:', err);
                liveErrorDisplay.textContent = `Mic/Audio Error: ${err.message}`;
                liveStatus.style.display = 'none';
            }
        });

        // Renamed Function + Added Processing Call
        stopAndProcessButton.addEventListener('click', () => stopRecordingAndProcess(false));

        async function stopRecordingAndProcess(isCalledOnError = false) {
            console.log("Stopping recording and processing...");
            stopAndProcessButton.disabled = true;

            // Stop Web Audio processing & visualization
            if (scriptNode) {
                 try { scriptNode.disconnect(); scriptNode = null; } catch(e) {}
            }
            if (liveStream) {
                liveStream.getTracks().forEach(track => track.stop());
                liveStream = null;
            }
            if (drawVisual) {
                cancelAnimationFrame(drawVisual);
                drawVisual = null;
                canvasCtx.fillStyle = '#333';
                canvasCtx.fillRect(0, 0, canvas.width, canvas.height);
            }
             if (source) { try { source.disconnect(); source = null; } catch(e) {} }
             if (analyser) { try { analyser.disconnect(); analyser = null; } catch(e) {} }

            // Close WebSocket
            if (liveWebSocket && (liveWebSocket.readyState === WebSocket.OPEN || liveWebSocket.readyState === WebSocket.CONNECTING)) {
                liveWebSocket.close(1000, "Client finished recording");
            }

            startRecordingButton.disabled = false;
            liveStatus.textContent = isCalledOnError ? 'Recording stopped due to error.' : 'Recording stopped. Processing transcript...';
            activeMicDisplay.textContent = '';

            if (isCalledOnError) {
                console.log("Skipping Gemini processing due to error during recording.");
                return; // Don't process if stopped due to error
            }

            // --- Process Accumulated Transcript --- 
            const finalTranscriptText = accumulatedFinalTranscript.trim();
            console.log(`Final Accumulated Transcript (${finalTranscriptText.length} chars):
${finalTranscriptText}`);

            if (!finalTranscriptText) {
                liveErrorDisplay.textContent = "No final transcript captured to process.";
                processingIndicator.style.display = 'none';
                return;
            }

            processingIndicator.style.display = 'block'; // Show processing indicator
            liveErrorDisplay.textContent = '';

            try {
                const response = await fetch('/process_transcript_text', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ transcript_text: finalTranscriptText })
                });

                processingIndicator.style.display = 'none';
                const data = await response.json();

                if (!response.ok) {
                    throw new Error(data.error || `HTTP error! status: ${response.status}`);
                }

                aiDraftArea.value = data.ai_generated_draft || '(No AI draft received)';
                confirmButton.disabled = false; // Enable saving
                liveStatus.textContent = 'Transcript processed. Review AI draft.';

            } catch (error) {
                console.error('Error processing transcript text:', error);
                liveErrorDisplay.textContent = `AI Processing Error: ${error.message}`;
                processingIndicator.style.display = 'none';
                confirmButton.disabled = true; // Keep save disabled on error
            }
            // --- End Processing --- 
        }

        // Helper function (Unchanged)
        function convertFloat32ToInt16(buffer) {
            let l = buffer.length;
            let buf = new Int16Array(l);
            while (l--) {
                buf[l] = Math.min(1, buffer[l]) * 0x7FFF; // Clamp to [-1, 1] then scale to [-32767, 32767]
            }
            return buf.buffer; // Return the underlying ArrayBuffer
        }

        // Visualization Function (Unchanged)
        function visualize() {
            if (!analyser) return;
            analyser.getByteFrequencyData(dataArray);
            canvasCtx.fillStyle = '#333';
            canvasCtx.fillRect(0, 0, canvas.width, canvas.height);
            const barWidth = (canvas.width / bufferLength) * 1.5;
            let barHeight;
            let x = 0;
            for (let i = 0; i < bufferLength; i++) {
                barHeight = dataArray[i] / 2;
                canvasCtx.fillStyle = `rgb(50, ${barHeight + 100}, 50)`;
                canvasCtx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
                x += barWidth + 1;
            }
            drawVisual = requestAnimationFrame(visualize);
        }

        // Save and Download Functions (Update save to use accumulated transcript)
         confirmButton.addEventListener('click', async () => {
            const finalNotes = aiDraftArea.value.trim();
            // Use the accumulated transcript variable
            const rawTranscript = accumulatedFinalTranscript.trim();

            if (!finalNotes) {
                liveErrorDisplay.textContent = 'Cannot save empty notes.'; // Use live error display
                return;
            }
            confirmButton.disabled = true;
            liveStatus.textContent = 'Saving consultation...';
            liveErrorDisplay.textContent = '';
            try {
                const response = await fetch(`/save_consultation/${patientId}`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        final_notes: finalNotes,
                        raw_transcript: rawTranscript, // Send accumulated transcript
                        // ai_summary: initialSummary, // Can remove if not needed
                        doctor_id: doctorId
                    })
                });
                const data = await response.json();
                if (!response.ok || !data.success) {
                     throw new Error(data.error || `HTTP error! status: ${response.status}`);
                }
                currentConsultationId = data.consultation_id;
                liveStatus.textContent = `Consultation saved (ID: ${currentConsultationId}). PDF available.`;
                downloadButton.style.display = 'inline-block';
                confirmButton.disabled = true;
                startRecordingButton.disabled = false;
            } catch (error) {
                console.error('Error saving consultation:', error);
                liveErrorDisplay.textContent = `Save Error: ${error.message}`;
                liveStatus.textContent = 'Failed to save.';
                confirmButton.disabled = false;
            }
        });

        downloadButton.addEventListener('click', () => {
            if (currentConsultationId) {
                window.location.href = `/download_pdf/${currentConsultationId}`;
            } else {
                liveErrorDisplay.textContent = 'Cannot download PDF. No consultation saved yet.';
            }
        });

    </script>
</body>
</html> 